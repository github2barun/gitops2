# List of pipelines to be loaded by Logstash
#
# This document must be a list of dictionaries/hashes, where the keys/values are pipeline settings.
# Default values for omitted settings are read from the `logstash.yml` file.
# When declaring multiple pipelines, each MUST have its own `pipeline.id`.
#
# Example of two pipelines:
#

#  *** Centralize Distributor Pipeline Configuration (NOTE: Mandatory to enable this if filebeat is enabled) ***
 - pipeline.id: distributor
   path.config: "/opt/seamless/conf/logstash/logstash_distributor.conf"

#  *** Pipeline for all transaction events ***
 - pipeline.id: data_lake
   path.config: "/opt/seamless/conf/logstash/logstash_data_lake.conf"

#  *** Pipeline for audit events  ***
 - pipeline.id: audit_data_lake
   path.config: "/opt/seamless/conf/logstash/logstash_audit.conf"

#  *** Pipeline for pending transaction events  ***
 - pipeline.id: pending_data_lake
   path.config: "/opt/seamless/conf/logstash/logstash_pending_transactions.conf"

#  *** Pipeline for DMS events  ***
 - pipeline.id: dms_data_lake
   path.config: "/opt/seamless/conf/logstash/logstash_dms_data_lake.conf"

#  *** Pipeline for scc moduel transaction ingestion for SCC  ***
 - pipeline.id: scc_data_lake
   path.config: "/opt/seamless/conf/logstash/logstash_scc_data_lake.conf"

#  *** Pipeline for txe transaction ingestion for SCC  ***
 - pipeline.id: scc_txe_ingestion
   path.config: "/opt/seamless/conf/logstash/logstash_scc_txe_ingestion.conf"

#  *** Pipeline for DMS Reseller insert & update events for SCC  ***
 - pipeline.id: reseller_data_lake
   path.config: "/opt/seamless/conf/logstash/logstash_reseller_data_lake.conf"

#  *** Pipeline for notification events  ***
 - pipeline.id: notification_data_lake
   path.config: "/opt/seamless/conf/logstash/logstash_notification_data_lake.conf"

# - pipeline.id: filebeat_data
#   path.config: "/opt/seamless/conf/logstash/filebeat.conf"
# - pipeline.id: kyc_data
#   path.config: "/opt/seamless/conf/logstash/logstash_kyc.conf"
# - pipeline.id: kyc_approve
#   path.config: "/opt/seamless/conf/logstash/logstash_approve_kyc.conf"
# - pipeline.id: is_data
#   path.config: "/opt/seamless/conf/logstash/logstash_is.conf"
# - pipeline.id: bank_payment
#   path.config: "/opt/seamless/conf/logstash/bank_payment_tdr.conf"
# - pipeline.id: vms_data
#   path.config: "/opt/seamless/conf/logstash/logstash_vms.conf"
# - pipeline.id: logstash_txe
#   path.config: "/opt/seamless/conf/logstash/logstash_txe.conf"
# - pipeline.id: reseller_balance
#   path.config: "/opt/seamless/conf/logstash/reseller_balance_tdr.conf"
# - pipeline.id: external_device_daily
#   path.config: "/opt/seamless/conf/logstash/logstash_data_external_device_daily.conf"
# - pipeline.id: external_device_monthly
#   path.config: "/opt/seamless/conf/logstash/logstash_data_external_device_monthly.conf"

# - pipeline.id: test
#   pipeline.workers: 1
#   pipeline.batch.size: 1
#   config.string: "input { generator {} } filter { sleep { time => 1 } } output { stdout { codec => dots } }"
# - pipeline.id: another_test
#   queue.type: persisted
#   path.config: "/tmp/logstash/*.config"
#
# Available options:
#
#   # name of the pipeline
#   pipeline.id: mylogs
#
#   # The configuration string to be used by this pipeline
#   config.string: "input { generator {} } filter { sleep { time => 1 } } output { stdout { codec => dots } }"
#
#   # The path from where to read the configuration text
#   path.config: "/etc/conf.d/logstash/myconfig.cfg"
#
#   # How many worker threads execute the Filters+Outputs stage of the pipeline
#   pipeline.workers: 1 (actually defaults to number of CPUs)
#
#   # How many events to retrieve from inputs before sending to filters+workers
#   pipeline.batch.size: 125
#
#   # How long to wait in milliseconds while polling for the next event
#   # before dispatching an undersized batch to filters+outputs
#   pipeline.batch.delay: 50
#
#   # Internal queuing model, "memory" for legacy in-memory based queuing and
#   # "persisted" for disk-based acked queueing. Defaults is memory
#   queue.type: memory
#
#   # If using queue.type: persisted, the page data files size. The queue data consists of
#   # append-only data files separated into pages. Default is 64mb
#   queue.page_capacity: 64mb
#
#   # If using queue.type: persisted, the maximum number of unread events in the queue.
#   # Default is 0 (unlimited)
#   queue.max_events: 0
#
#   # If using queue.type: persisted, the total capacity of the queue in number of bytes.
#   # Default is 1024mb or 1gb
#   queue.max_bytes: 1024mb
#
#   # If using queue.type: persisted, the maximum number of acked events before forcing a checkpoint
#   # Default is 1024, 0 for unlimited
#   queue.checkpoint.acks: 1024
#
#   # If using queue.type: persisted, the maximum number of written events before forcing a checkpoint
#   # Default is 1024, 0 for unlimited
#   queue.checkpoint.writes: 1024
#
#   # If using queue.type: persisted, the interval in milliseconds when a checkpoint is forced on the head page
#   # Default is 1000, 0 for no periodic checkpoint.
#   queue.checkpoint.interval: 1000
#
#   # Enable Dead Letter Queueing for this pipeline.
#   dead_letter_queue.enable: false
#
#   If using dead_letter_queue.enable: true, the maximum size of dead letter queue for this pipeline. Entries
#   will be dropped if they would increase the size of the dead letter queue beyond this setting.
#   Default is 1024mb
#   dead_letter_queue.max_bytes: 1024mb
#
#   If using dead_letter_queue.enable: true, the directory path where the data files will be stored.
#   Default is path.data/dead_letter_queue
#
#   path.dead_letter_queue:
