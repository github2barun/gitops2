#Logstash configuration
input {
  ######### Uncomment this pipeline address if we use FileBeat and not Logstash directly.
  #pipeline { address => scc_data_lake_pipeline }

  ######### comment out below section if we don't use logstash directly but through Filebeat.
  file {
      start_position => "beginning"
      path =>  ["/var/seamless/log/scc-data-aggregator/scc-data-aggregator-data.dump*","/var/seamless/log/scc-commission-engine/scc-commission-engine-data.dump*","/var/seamless/log/scc-payout-engine/scc-payout-engine-data.dump*","/var/seamless/log/scc-engine-core/scc-engine-data.dump*"]
      sincedb_path => "/var/seamless/log/sincedb_scc_data_lake.log"
    }
  #######################################################################

}

#Filter out the logs before inserting to data lake
filter  {
    json{
        source => "message"
    }

    #verify if transaction contains valid json strcuture or not
    if "_jsonparsefailure" not in [tags] {

#       if ([eventName] in ["getResellerInfo"]) {
#           drop{}
#       }

        mutate {
          convert => { "rootComponent" => "boolean" }

          # SCC fields for aggregation
          convert => { "commission.commissionAmount" => "float" }
          convert => { "payout.netPayoutAmount" => "float" }
          convert => { "sccAggregator.totalRawTransactions" => "float" }
          convert => { "sccAggregator.totalRawTransactionAmount" => "float" }

          add_field => {
               "originalLastUpdatedTimestamp" => "%{lastUpdatedTimestamp}"
          }
        }

        date {
                match => ["timestamp","yyyy-MM-dd HH:mm:ss"]
                timezone => "UTC"
                target => "timestamp"
        }

        ruby {
               code => "
                 require 'date'
                 if event.get('componentName')
                     componentName = event.get('componentName') + '.eventName'
                     if  event.get('eventName')
                        eventName = event.get('eventName')
                        event.set(componentName,eventName)
                     end
                 end
                 week_n = event.get('timestamp').time.strftime '%V'
                 month_n = event.get('timestamp').time.strftime '%m'
                 year_n = event.get('timestamp').time.strftime '%Y'
                 if(week_n == '01' && month_n == '12')
                     year_n = (year_n.to_i + 1)
                     week_num = year_n.to_s + 'w' + week_n.to_s
                 else if (month_n == '01' && week_n.to_i > 50)
                     year_n = (year_n.to_i - 1)
                     week_num = year_n.to_s + 'w' + week_n.to_s
                 else
                     week_num = year_n + 'w' + week_n
                 end
                 end
                 event.set('[@metadata][week_num]', week_num)
                 month_n = month_n.to_i
                 event.set('[@metadata][month_num]', month_n)
                 if(month_n >=1 && month_n <=3)
                    event.set('[@metadata][quarter]', 'Q1')
                 else if(month_n >=4 && month_n <=6)
                    event.set('[@metadata][quarter]', 'Q2')
                 else if(month_n >=7 && month_n <=9)
                    event.set('[@metadata][quarter]', 'Q3')
                 else
                    event.set('[@metadata][quarter]', 'Q4')
                 end
                 end
                 end
                 "
        }

        prune {
               blacklist_names => ["log","tags","message","path","@version","host","ecs","input","cloud"]
         }

#        if ([rootComponent]) {
#             #Remove all fields below inside blacklist_names list.
#             prune {
#               blacklist_names => ["log","tags","message","path","@version","host","ecs","input","cloud"]
#             }
#        } else {
#             #Remove all fields except below whitelist_names list.
#             prune {
#                 whitelist_names => ["^timestamp$","%{componentName}","transactionNumber"]
#                 interpolate => true
#             }
#        }

        mutate {
             add_field => {
                 "Quarter" => "%{[@metadata][quarter]}"
                 "month" => "%{[@metadata][month_num]}"
             }
        }

        ruby {
             code => "event.set('lastUpdatedTimestamp', (event.get('@timestamp').to_f*1000).to_i)"
        }

        ruby {
            path => "/opt/seamless/conf/logstash/ruby/flattenJSON.rb"
            script_params => { "field" => "sccAggregator.additionalFields" }
        }

  } else {
       #data-feed json is invalid, so prepare index name with @timestamp
       #Doc Id of skipped txn would be random number generated by ES
       prune {
             blacklist_names => ["agent","path","@version","host","ecs","input","cloud"]
       }
       ruby {
             code => "
             puts 'Transaction skipped due to invalid JSON format with @timestamp:'
             puts event.get('@timestamp')
             require 'date'
             week_n = event.get('@timestamp').time.strftime '%V'
             month_n = event.get('@timestamp').time.strftime '%m'
             year_n = event.get('@timestamp').time.strftime '%Y'
             if(week_n == '01' && month_n == '12')
                 year_n = (year_n.to_i + 1)
                 week_num = year_n.to_s + 'w' + week_n.to_s
             else if (month_n == '01' && week_n.to_i > 50)
                 year_n = (year_n.to_i - 1)
                 week_num = year_n.to_s + 'w' + week_n.to_s
             else
                 week_num = year_n + 'w' + week_n
             end
             end
             event.set('[@metadata][week_num]', week_num)
             "
       }
  }
}

output {
    if "_jsonparsefailure" in [tags] {
        #if data-feed json is not in correct format, insert it to skipped_txn index
        elasticsearch {
            action => "index"
            hosts => [ "localhost:9200" ]
            #user => elastic
            #password => seamless2021
            index => "skipped_txn_%{[@metadata][week_num]}"
        }
    }
    else {
        elasticsearch {
            action => "update"
            hosts => [ "localhost:9200" ]
            #user => elastic
            #password => seamless2021
            index => "data_lake_%{[@metadata][week_num]}"
            document_id => "%{transactionNumber}"
            doc_as_upsert => true
        }
    }
    stdout {codec => rubydebug}
}